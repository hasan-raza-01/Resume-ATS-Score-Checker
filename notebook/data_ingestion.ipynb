{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca9748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12add06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hasan\\\\Artificial-Intelligence\\\\projects\\\\Resume ATS Score Checker\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9318aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becadc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hasan\\\\Artificial-Intelligence\\\\projects\\\\Resume ATS Score Checker'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae1bd3",
   "metadata": {},
   "source": [
    "## config.raw"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3a5e242",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ROOT_DIR: artifacts \n",
    "\n",
    "DATA: \n",
    "    ROOT_DIR: data \n",
    "    INGESTION: \n",
    "        ROOT_DIR: ingestion \n",
    "        RAW_DATA_DIR: feature_store \n",
    "        PARSED_DATA_DIR: ingested \n",
    "        FINAL_DATA_DIR: final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a07c2",
   "metadata": {},
   "source": [
    "## constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e53c81",
   "metadata": {},
   "source": [
    "##### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561f44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field \n",
    "\n",
    "\n",
    "class DataIngestionConstants(BaseModel):\n",
    "    ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    DATA_ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    INGESTION_ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    RAW_DATA_DIR_NAME: str = Field(frozen=True) \n",
    "    PARSED_DATA_DIR_NAME: str = Field(frozen=True) \n",
    "    FINAL_DATA_DIR_NAME: str = Field(frozen=True) \n",
    "\n",
    "\n",
    "__all__ = [\"DataIngestionConstants\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e648e",
   "metadata": {},
   "source": [
    "##### values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740058ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .schema import *\n",
    "from src.ats.exception import CustomException \n",
    "from typing import List, Tuple, Dict\n",
    "from box import ConfigBox\n",
    "import sys \n",
    "\n",
    "\n",
    "def __ing__(CONFIG:ConfigBox):\n",
    "    return DataIngestionConstants(\n",
    "        ROOT_DIR_NAME = CONFIG.ROOT_DIR, \n",
    "        DATA_ROOT_DIR_NAME = CONFIG.DATA.ROOT_DIR, \n",
    "        INGESTION_ROOT_DIR_NAME = CONFIG.DATA.INGESTION.ROOT_DIR, \n",
    "        RAW_DATA_DIR_NAME = CONFIG.DATA.INGESTION.RAW_DATA_DIR, \n",
    "        PARSED_DATA_DIR_NAME = CONFIG.DATA.INGESTION.PARSED_DATA_DIR, \n",
    "        FINAL_DATA_DIR_NAME = CONFIG.DATA.INGESTION.FINAL_DATA_DIR \n",
    "    )\n",
    "\n",
    "avl_cons = [\"DataIngestion\", ]\n",
    "process = {\n",
    "    \"DataIngestion\":__ing__\n",
    "} \n",
    "\n",
    "def load(config:ConfigBox, name: str | List[str] | Tuple[str]) -> Dict: \n",
    "    \"\"\"loads respective constants for the given name\n",
    "\n",
    "    Args:\n",
    "        config (ConfigBox): configuration for the object\n",
    "        name (str | List[str] | Tuple[str]): name of required object  \n",
    "\n",
    "        Note: Available name --> DataIngestion, \n",
    "\n",
    "    Raises:\n",
    "        CustomException: Error shows with file name, line no and error message\n",
    "\n",
    "    Returns:\n",
    "        Dict: key = name of object used to load given in variable \\'name\\', \n",
    "        \n",
    "              value = Object of the name used to load,\n",
    "\n",
    "              example:\n",
    "              output = load(config, \"DataIngestion\")\n",
    "              output = { \"DataIngestion\" : DataIngestionConstants } \n",
    "              data_ingestion_constants = output[\"DataIngestion\"] \n",
    "    \"\"\"\n",
    "    reqs = []\n",
    "    try:\n",
    "        # validate type   \n",
    "        if isinstance(name, str):\n",
    "            reqs.append(name) \n",
    "        elif isinstance(name, List) or isinstance(name, Tuple):\n",
    "            reqs += name \n",
    "        else:\n",
    "            ValueError(f\"Unsupported type {{{type(name)}}} for variable {{name}}\") \n",
    "\n",
    "        # validate values \n",
    "        for req in reqs:\n",
    "            if req not in avl_cons:\n",
    "                ValueError(f\"Unknown value provided in variable \\'name\\', {req}, name can only have values from {avl_cons}\") \n",
    "\n",
    "        # run respective functions and return the output \n",
    "        output = {}\n",
    "        for req in reqs: \n",
    "            func = process[req] \n",
    "            output[req] = func(config)\n",
    "\n",
    "        return output\n",
    "    except Exception as e: \n",
    "        raise CustomException(e, sys) \n",
    "\n",
    "\n",
    "__all__ = [\"load\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ab5e9",
   "metadata": {},
   "source": [
    "##### __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5034ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .values import *\n",
    "from src.ats.utils import load_yaml\n",
    "from typing import Dict \n",
    "\n",
    "\n",
    "\n",
    "CONFIG = load_yaml(\"src\\\\ats\\\\config\\\\raw\\\\config.yaml\") \n",
    "\n",
    "def load_constants(name: str | list[str] | tuple[str]) -> Dict:\n",
    "    \"\"\"loads respective constants for the given name\n",
    "\n",
    "    Args:\n",
    "        name (str | list[str] | tuple[str]): name of required object \n",
    "\n",
    "        Note: Available name --> DataIngestion, \n",
    "\n",
    "    Returns:\n",
    "        Dict: key = name of object used to load given in variable \\'name\\', \n",
    "\n",
    "              value = Object of the name used to load,\n",
    "\n",
    "              example:\n",
    "              output = load_constants(\"DataIngestion\")\n",
    "              output = { \"DataIngestion\" : DataIngestionConstants } \n",
    "              data_ingestion_constants = output[\"DataIngestion\"] \n",
    "    \"\"\"\n",
    "    return load(CONFIG, name)\n",
    "\n",
    "\n",
    "__all__ = [\"load_constants\", \"CONFIG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee3a08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR_NAME:artifacts\n",
      "DATA_ROOT_DIR_NAME:data\n",
      "INGESTION_ROOT_DIR_NAME:ingestion\n",
      "RAW_DATA_DIR_NAME:raw\n",
      "PARSED_DATA_DIR_NAME:parsed\n",
      "FINAL_DATA_DIR_NAME:final\n"
     ]
    }
   ],
   "source": [
    "constants = load_constants(\"DataIngestion\")[\"DataIngestion\"]\n",
    "print(f\"ROOT_DIR_NAME:{constants.ROOT_DIR_NAME}\")\n",
    "print(f\"DATA_ROOT_DIR_NAME:{constants.DATA_ROOT_DIR_NAME}\")\n",
    "print(f\"INGESTION_ROOT_DIR_NAME:{constants.INGESTION_ROOT_DIR_NAME}\")\n",
    "print(f\"RAW_DATA_DIR_NAME:{constants.RAW_DATA_DIR_NAME}\")\n",
    "print(f\"PARSED_DATA_DIR_NAME:{constants.PARSED_DATA_DIR_NAME}\") \n",
    "print(f\"FINAL_DATA_DIR_NAME:{constants.FINAL_DATA_DIR_NAME}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d7b49",
   "metadata": {},
   "source": [
    "## entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5eea6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel \n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "class DataIngestion(BaseModel):\n",
    "    ROOT_DIR_PATH: Path \n",
    "    DATA_ROOT_DIR_PATH: Path \n",
    "    INGESTION_ROOT_DIR_PATH: Path \n",
    "    RAW_DATA_DIR_PATH: Path \n",
    "    PARSED_DATA_DIR_PATH: Path  \n",
    "    FINAL_DATA_DIR_PATH: Path\n",
    "\n",
    "\n",
    "__all__ = [\"DataIngestion\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27737b19",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685c101",
   "metadata": {},
   "source": [
    "##### builder/__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978870a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.ats.constants import load_constants \n",
    "# update __all__ \n",
    "\n",
    "# from src.ats.constants import * \n",
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "\n",
    "constants = load_constants([\"DataIngestion\", ])\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    CONFIG = constants[\"DataIngestion\"]\n",
    "    ROOT_DIR_PATH = Path(CONFIG.ROOT_DIR_NAME)\n",
    "    DATA_ROOT_DIR_PATH = Path(os.path.join(ROOT_DIR_PATH, CONFIG.DATA_ROOT_DIR_NAME))\n",
    "    INGESTION_ROOT_DIR_PATH = Path(os.path.join(DATA_ROOT_DIR_PATH, CONFIG.INGESTION_ROOT_DIR_NAME))\n",
    "    RAW_DATA_DIR_PATH = Path(os.path.join(INGESTION_ROOT_DIR_PATH, CONFIG.RAW_DATA_DIR_NAME))\n",
    "    PARSED_DATA_DIR_PATH = Path(os.path.join(INGESTION_ROOT_DIR_PATH, CONFIG.PARSED_DATA_DIR_NAME)) \n",
    "    FINAL_DATA_DIR_PATH = Path(os.path.join(INGESTION_ROOT_DIR_PATH, CONFIG.FINAL_DATA_DIR_NAME)) \n",
    "\n",
    "\n",
    "__all__ = [\"DataIngestionConfig\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e744968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR_PATH: artifacts\n",
      "DATA_ROOT_DIR_PATH: artifacts\\data\n",
      "INGESTION_ROOT_DIR_PATH: artifacts\\data\\ingestion\n",
      "RAW_DATA_DIR_PATH: artifacts\\data\\ingestion\\raw\n",
      "PARSED_DATA_DIR_PATH: artifacts\\data\\ingestion\\parsed\n",
      "FINAL_DATA_DIR_PATH: artifacts\\data\\ingestion\\final\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROOT_DIR_PATH: {DataIngestionConfig.ROOT_DIR_PATH}\")\n",
    "print(f\"DATA_ROOT_DIR_PATH: {DataIngestionConfig.DATA_ROOT_DIR_PATH}\")\n",
    "print(f\"INGESTION_ROOT_DIR_PATH: {DataIngestionConfig.INGESTION_ROOT_DIR_PATH}\")\n",
    "print(f\"RAW_DATA_DIR_PATH: {DataIngestionConfig.RAW_DATA_DIR_PATH}\")\n",
    "print(f\"PARSED_DATA_DIR_PATH: {DataIngestionConfig.PARSED_DATA_DIR_PATH}\")\n",
    "print(f\"FINAL_DATA_DIR_PATH: {DataIngestionConfig.FINAL_DATA_DIR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a20544",
   "metadata": {},
   "source": [
    "##### __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3bbb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .builder import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5eb27",
   "metadata": {},
   "source": [
    "## components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26106b41",
   "metadata": {},
   "source": [
    "##### data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.docx import partition_docx \n",
    "from unstructured.partition.html import partition_html \n",
    "from src.ats.utils import save_file, create_dirs\n",
    "from src.ats.exception import CustomException \n",
    "from src.ats import logging \n",
    "from pydantic import BaseModel \n",
    "from fastapi import UploadFile \n",
    "from string import punctuation \n",
    "# from src.ats.entity import * \n",
    "from pathlib import Path \n",
    "from typing import Dict \n",
    "import sys \n",
    "\n",
    "\n",
    "class DataIngestionComponents(BaseModel): \n",
    "    data_ingestion_config: DataIngestion \n",
    "\n",
    "    def __load(self, files:List[UploadFile]) -> Dict[str, Path]: \n",
    "        \"\"\"loads data, stores locally and returns name with path\n",
    "\n",
    "        Args:\n",
    "            files (List[UploadFile]): list object of fastapi.UploadFile / files that have been uploaded\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if no files have been provided as argument\n",
    "\n",
    "        Returns:\n",
    "            Dict:\n",
    "            key = name of file \n",
    "\n",
    "            value = path of file \n",
    "\n",
    "            example:\n",
    "            output = __load(files)\n",
    "            output = {\n",
    "                \"xyz.pdf\" : \"path\\\\to\\\\the\\\\file\", \n",
    "                \"abc.docx\": \"path\\\\to\\\\the\\\\file\", \n",
    "                ...\n",
    "            } \n",
    "            name = output.keys()[0]\n",
    "            path = output[name]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In __load\")\n",
    "            files_len = len(files)\n",
    "            if files_len > 0: \n",
    "                output = {} \n",
    "                for file in files:\n",
    "                    file_name = file.filename \n",
    "                    file_name = file_name.strip().lower() \n",
    "                    logging.info(f\"working with \\'{file_name}\\'\")\n",
    "                    path = os.path.join(self.data_ingestion_config.RAW_DATA_DIR_PATH, file_name)\n",
    "                    # save file to local  \n",
    "                    save_file(file.file.read(), path)\n",
    "                    output[file_name] = path \n",
    "                    logging.info(f\"\\'{file_name}\\' saved at \\'{path}\\'\")\n",
    "                else: \n",
    "                    raise ValueError(f\"{len(files_len)} files recieved.\")\n",
    "            logging.info(\"Out __load\")\n",
    "            return output \n",
    "        except Exception as e: \n",
    "            logging.error(e) \n",
    "            raise CustomException(e, sys) \n",
    "        \n",
    "    def __parse(self, info:Dict[str, Path]) -> Dict[str, str]: \n",
    "        \"\"\"parse the data of file through unstructured, supported extentions ---> [ .pdf, .docx, .html ]\n",
    "\n",
    "        Args:\n",
    "            info (Dict[str, Path]): \n",
    "\n",
    "                key = name of file \n",
    "\n",
    "                value = path of file \n",
    "\n",
    "                example:\n",
    "                info = {\n",
    "                    \"xyz.pdf\" : \"path\\\\to\\\\the\\\\file\", \n",
    "                    \"abc.docx\": \"path\\\\to\\\\the\\\\file\", \n",
    "                    ...\n",
    "                } \n",
    "                output = __parse(info)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if provided file with uncompatable format as argument \n",
    "\n",
    "        Returns:\n",
    "            Dict: \n",
    "            key = name of file \n",
    "\n",
    "            value = string object of parsed data  \n",
    "\n",
    "            example:\n",
    "            output = __parse(info)\n",
    "            output = {\n",
    "                \"xyz.pdf\" : \"string_parsed_data_of_xyz.pdf\", \n",
    "                \"abc.docx\": \"string_parsed_data_of_abc.docx\", \n",
    "                ...\n",
    "            } \n",
    "            name = output.keys()[0]\n",
    "            data = output[name] \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In __parse\")\n",
    "            output = {}\n",
    "            for file_name in info.keys():\n",
    "                logging.info(f\"parsing \\'{file_name}\\'\")\n",
    "                ext = os.path.splitext(file_name)[1].lower()\n",
    "                # get partitioner \n",
    "                if ext == \".pdf\":\n",
    "                    partition = partition_pdf \n",
    "                    partitioner_type = \"partition_pdf\"\n",
    "                elif ext == \".docx\":\n",
    "                    partition = partition_docx\n",
    "                    partitioner_type = \"partition_docx\"\n",
    "                elif ext == \".html\":\n",
    "                    partition = partition_html\n",
    "                    partitioner_type = \"partition_html\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file type: {file_name}\") \n",
    "                del ext\n",
    "                logging.info(f\"using \\'{partitioner_type}\\'\") \n",
    "                del partitioner_type\n",
    "                path = info[file_name]\n",
    "                logging.info(f\"path of file for partitioning \\'{path}\\'\") \n",
    "                elements = partition(path)\n",
    "                del partition \n",
    "                elements_string = \"\\n\\n\".join([str(el) for el in elements]) \n",
    "                del elements \n",
    "                logging.info(\"partitioning complete.\")\n",
    "                # save file to local \n",
    "                path = os.path.join(self.data_ingestion_config.PARSED_DATA_DIR_PATH, file_name)\n",
    "                save_file(elements_string, path)\n",
    "                del path \n",
    "                output[file_name] = elements_string \n",
    "                del elements_string \n",
    "            logging.info(\"Out __parse\")\n",
    "            return output \n",
    "        except Exception as e: \n",
    "            logging.error(e) \n",
    "            raise CustomException(e, sys) \n",
    "        \n",
    "    def __clean(self, info:Dict[str, str]) -> Dict[str, str]: \n",
    "        \"\"\"removes punctuations from parsed data \n",
    "\n",
    "        Args:\n",
    "            info (Dict[str, str]): \n",
    "            \n",
    "                key = name of file \n",
    "\n",
    "                value = string object of parsed data  \n",
    "\n",
    "                example:\n",
    "                info = {\n",
    "                    \"xyz.pdf\" : \"string_parsed_data_of_xyz.pdf\", \n",
    "                    \"abc.docx\": \"string_parsed_data_of_abc.docx\", \n",
    "                    ...\n",
    "                } \n",
    "                output = __clean(info)\n",
    "\n",
    "        Returns:\n",
    "            Dict: \n",
    "            key = name of file \n",
    "\n",
    "            value = cleaned string object of parsed data  \n",
    "\n",
    "            example:\n",
    "            output = __clean(info)\n",
    "            output = {\n",
    "                \"xyz.pdf\" : \"cleaned_string_parsed_data_of_xyz.pdf\", \n",
    "                \"abc.docx\": \"cleaned_string_parsed_data_of_abc.docx\", \n",
    "                ...\n",
    "            } \n",
    "            name = output.keys()[0]\n",
    "            data = output[name] \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In __clean\") \n",
    "            output = {} \n",
    "            for file_name in info.keys():\n",
    "                logging.info(f\"cleaning \\'{file_name}\\'\")\n",
    "                logging.info(f\"cleaning \\'{file_name}\\'\")\n",
    "                new_line_char = \" mmmmmmm \" \n",
    "                elements_string = info[file_name]\n",
    "                elements_string = elements_string.replace(\"\\n\", new_line_char).replace(\"|\", \",\") \n",
    "                elements_string = re.sub(r'–', '-', elements_string) \n",
    "                for i in punctuation: \n",
    "                    elements_string = elements_string.replace(i, \" \") \n",
    "                elements_string = \" \".join(elements_string.split())\n",
    "                elements_string = elements_string.replace(new_line_char.strip(), \"\\n\") \n",
    "                del new_line_char \n",
    "                # save file to local \n",
    "                path = os.path.join(self.data_ingestion_config.FINAL_DATA_DIR_PATH, file_name)\n",
    "                save_file(elements_string, path) \n",
    "                output[file_name] = elements_string \n",
    "                del elements_string \n",
    "            logging.info(\"Out __clean\") \n",
    "        except Exception as e: \n",
    "            logging.error(e) \n",
    "            raise CustomException(e, sys) \n",
    "        \n",
    "    def _main(self, files: List[UploadFile]) -> Dict[str, str]: \n",
    "        \"\"\"runs data ingestion components \n",
    "\n",
    "        Args:\n",
    "            files (List[UploadFile]): list object of fastapi.UploadFile / files that have been uploaded\n",
    "\n",
    "        Returns:\n",
    "            Dict: \n",
    "            key = name of file \n",
    "\n",
    "            value = cleaned string object of parsed data  \n",
    "\n",
    "            example:\n",
    "            output = __clean(info)\n",
    "            output = {\n",
    "                \"xyz.pdf\" : \"cleaned_string_parsed_data_of_xyz.pdf\", \n",
    "                \"abc.docx\": \"cleaned_string_parsed_data_of_abc.docx\", \n",
    "                ...\n",
    "            } \n",
    "            name = output.keys()[0]\n",
    "            data = output[name] \n",
    "        \"\"\"\n",
    "        # create required dir's \n",
    "        create_dirs(self.data_ingestion_config.ROOT_DIR_PATH)\n",
    "        create_dirs(self.data_ingestion_config.DATA_ROOT_DIR_PATH)\n",
    "        create_dirs(self.data_ingestion_config.INGESTION_ROOT_DIR_PATH)\n",
    "        create_dirs(self.data_ingestion_config.RAW_DATA_DIR_PATH)\n",
    "        create_dirs(self.data_ingestion_config.PARSED_DATA_DIR_PATH)\n",
    "        create_dirs(self.data_ingestion_config.FINAL_DATA_DIR_PATH)\n",
    "        # return output \n",
    "        return self.__clean(self.__parse(self.__load(files))) \n",
    "    \n",
    "__all__ = [\"DataIngestionComponents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89060388",
   "metadata": {},
   "source": [
    "##### __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.ats.components.data_ingestion import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c12dc",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d4465",
   "metadata": {},
   "source": [
    "##### __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.ats.components import * \n",
    "# from src.ats.config.builder import * \n",
    "from dataclasses import dataclass \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataIngestionPipeline: \n",
    "    def _run(self, files: List[UploadFile]) -> Dict[str, str]: \n",
    "        \"\"\"runs data ingestion pipeline and returns the output\n",
    "        \"\"\"\n",
    "        self.config = DataIngestionConfig()\n",
    "        self.components = DataIngestionComponents(self.config) \n",
    "        return self.components._main(files) \n",
    "    \n",
    "__all__ = [\"DataIngestionPipeline\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009449fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ats-score-checker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
