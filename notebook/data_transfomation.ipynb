{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e862fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3797b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f05a23",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38990d",
   "metadata": {},
   "source": [
    "### raw/config.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8912f42",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ROOT_DIR: artifacts \n",
    "\n",
    "REPORTS: \n",
    "    ROOT_DIR: reports\n",
    "    \n",
    "STRUCTURED_TRAINING:\n",
    "    ROOT_DIR: train_data\n",
    "\n",
    "DATA: \n",
    "    ROOT_DIR: data \n",
    "\n",
    "    INGESTION: \n",
    "        ROOT_DIR: ingestion \n",
    "        RAW_DATA_DIR: raw \n",
    "        SCHEMA_DATA_DIR: schema \n",
    "        \n",
    "    TRANSFORMATION: \n",
    "        ROOT_DIR: transformation \n",
    "        PARSED_DATA_DIR: parsed \n",
    "        STRUCTURED_DATA_DIR: structured "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926c54f",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323240fa",
   "metadata": {},
   "source": [
    "### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update __all__ \n",
    "\n",
    "from pydantic import BaseModel, Field \n",
    "\n",
    "\n",
    "class Constants:\n",
    "    ...\n",
    "\n",
    "class DataIngestionConstants(BaseModel):\n",
    "    ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    REPORTS_DIR_NAME: str = Field(frozen=True)\n",
    "    DATA_ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    INGESTION_ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    RAW_DATA_DIR_NAME: str = Field(frozen=True)\n",
    "    SCHEMA_DATA_DIR_NAME: str = Field(frozen=True)\n",
    "\n",
    "class DataTransformationConstants(BaseModel):\n",
    "    ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    DATA_ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    TRANSFORMATION_ROOT_DIR_NAME: str = Field(frozen=True) \n",
    "    PARSED_DATA_DIR_NAME: str = Field(frozen=True)\n",
    "    STRUCTURED_DATA_DIR_NAME: str = Field(frozen=True)\n",
    "    TRAIN_DATA_DIR_NAME: str = Field(frozen=True)\n",
    "\n",
    "\n",
    "__all__ = [\"DataIngestionConstants\", \"DataTransformationConstants\", \"Constants\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ee54d",
   "metadata": {},
   "source": [
    "### values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 things needed to be updated at a time [avl_cons, process, Note: Available name] \n",
    "# Note is inside docstring of function 'load' from this file and 'load_constants' inside __int__.py \n",
    "# update __all__ \n",
    "\n",
    "# from src.ats.constants.schema import *\n",
    "from src.ats.exception import CustomException \n",
    "from typing import List, Tuple, Dict\n",
    "from box import ConfigBox\n",
    "import sys \n",
    "\n",
    "\n",
    "def __ing__(CONFIG:ConfigBox) -> Constants:\n",
    "    return DataIngestionConstants(\n",
    "        ROOT_DIR_NAME = CONFIG.ROOT_DIR, \n",
    "        REPORTS_DIR_NAME = CONFIG.REPORTS.ROOT_DIR,\n",
    "        DATA_ROOT_DIR_NAME = CONFIG.DATA.ROOT_DIR, \n",
    "        INGESTION_ROOT_DIR_NAME = CONFIG.DATA.INGESTION.ROOT_DIR, \n",
    "        RAW_DATA_DIR_NAME = CONFIG.DATA.INGESTION.RAW_DATA_DIR,\n",
    "        SCHEMA_DATA_DIR_NAME = CONFIG.DATA.INGESTION.SCHEMA_DATA_DIR\n",
    "    )\n",
    "\n",
    "def __transform__(CONFIG:ConfigBox) -> Constants:\n",
    "    return DataTransformationConstants(\n",
    "        ROOT_DIR_NAME = CONFIG.ROOT_DIR , \n",
    "        DATA_ROOT_DIR_NAME = CONFIG.DATA.ROOT_DIR , \n",
    "        TRANSFORMATION_ROOT_DIR_NAME = CONFIG.DATA.TRANSFORMATION.ROOT_DIR , \n",
    "        PARSED_DATA_DIR_NAME = CONFIG.DATA.TRANSFORMATION.PARSED_DATA_DIR ,\n",
    "        STRUCTURED_DATA_DIR_NAME = CONFIG.DATA.TRANSFORMATION.STRUCTURED_DATA_DIR ,\n",
    "        TRAIN_DATA_DIR_NAME = CONFIG.STRUCTURED_TRAINING.ROOT_DIR,\n",
    "    )\n",
    "\n",
    "avl_cons = [\n",
    "    \"dataingestion\", \n",
    "    \"datatransformation\", \n",
    "]\n",
    "process = {\n",
    "    \"dataingestion\":__ing__,\n",
    "    \"datatransformation\":__transform__\n",
    "} \n",
    "\n",
    "def load(config:ConfigBox, name: str | List[str] | Tuple[str]) -> Dict: \n",
    "    \"\"\"loads respective constants for the given name\n",
    "\n",
    "    Args:\n",
    "        config (ConfigBox): configuration for the object\n",
    "        name (str | List[str] | Tuple[str]): name of required object  \n",
    "\n",
    "        Note: Available names --> DataIngestion, DataTransformation,  \n",
    "\n",
    "    Raises:\n",
    "        CustomException: Error shows with file name, line no and error message\n",
    "\n",
    "    Returns:\n",
    "        Dict: key = name of object used to load given in variable \\'name\\', \n",
    "        \n",
    "              value = Object of the name used to load,\n",
    "\n",
    "              example:\n",
    "              output = load(config, \"DataIngestion\")\n",
    "              output = { \"DataIngestion\" : DataIngestionConstants } \n",
    "              data_ingestion_constants = output[\"DataIngestion\"] \n",
    "    \"\"\"\n",
    "    reqs:List[str] = []\n",
    "    try:\n",
    "        # validate type   \n",
    "        if isinstance(name, str):\n",
    "            reqs.append(name) \n",
    "        elif isinstance(name, List) or isinstance(name, Tuple):\n",
    "            reqs += name \n",
    "        else:\n",
    "            ValueError(f\"Unsupported type {{{type(name)}}} for variable {{name}}\") \n",
    "\n",
    "        # validate values \n",
    "        for req in reqs:\n",
    "            req = req.strip().lower()\n",
    "            if req not in avl_cons:\n",
    "                ValueError(f\"Unknown value provided in variable \\'name\\', {req}, name can only have values from {avl_cons}\") \n",
    "\n",
    "        # run respective functions and return the output \n",
    "        output = {}\n",
    "        for req in reqs: \n",
    "            func = process[req] \n",
    "            output[req] = func(config)\n",
    "\n",
    "        return output\n",
    "    except Exception as e: \n",
    "        raise CustomException(e, sys) \n",
    "    \n",
    "\n",
    "__all__ = [\"load\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006e0f8",
   "metadata": {},
   "source": [
    "### __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd691cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update required on 'Note: Available name' inside docstring of fuction 'load_constants' \n",
    "# update __all__ if needed\n",
    "\n",
    "# from src.ats.constants.values import *\n",
    "from src.ats.utils import load_yaml\n",
    "from typing import Dict \n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "CONFIG = load_yaml(os.path.join(\"src\", \"ats\", \"config\", \"raw\", \"config.yaml\")) \n",
    "\n",
    "def load_constants(name: str | list[str] | tuple[str]) -> Dict:\n",
    "    \"\"\"loads respective constants for the given name\n",
    "\n",
    "    Args:\n",
    "        name (str | list[str] | tuple[str]): name of required object \n",
    "\n",
    "        Note: Available names --> DataIngestion, DataTransformation,\n",
    "\n",
    "    Returns:\n",
    "        Dict: key = name of object used to load given in variable \\'name\\', \n",
    "\n",
    "              value = Object of the name used to load,\n",
    "\n",
    "              example:\n",
    "              output = load_constants(\"DataIngestion\")\n",
    "              output = { \"DataIngestion\" : DataIngestionConstants } \n",
    "              data_ingestion_constants = output[\"DataIngestion\"] \n",
    "    \"\"\"\n",
    "    return load(CONFIG, name)\n",
    "\n",
    "\n",
    "__all__ = [\"load_constants\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b269e10d",
   "metadata": {},
   "source": [
    "# entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update __all__ \n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "class DataIngestion(BaseModel):\n",
    "    ROOT_DIR_PATH: Path\n",
    "    REPORTS_DIR_PATH: Path\n",
    "    DATA_ROOT_DIR_PATH: Path\n",
    "    INGESTION_ROOT_DIR_PATH: Path\n",
    "    RAW_DATA_DIR_PATH: Path\n",
    "    SCHEMA_DATA_DIR_PATH: Path\n",
    "\n",
    "class DataTransformation(BaseModel):\n",
    "    ROOT_DIR_PATH: Path\n",
    "    DATA_ROOT_DIR_PATH: Path\n",
    "    TRANSFORMATION_ROOT_DIR_PATH: Path\n",
    "    PARSED_DATA_DIR_PATH: Path\n",
    "    STRUCTURED_DATA_DIR_PATH: Path\n",
    "    TRAIN_DATA_DIR_PATH: Path\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\"DataIngestion\", \"DataTransformation\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ee3c3",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b5c37",
   "metadata": {},
   "source": [
    "### builder/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.ats.constants import load_constants \n",
    "# update __all__ also inside __init__ \n",
    "\n",
    "# from src.ats.constants import * \n",
    "# from src.ats.entity import *\n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "\n",
    "constants = load_constants([\"DataIngestion\", \"DataTransformation\"])\n",
    "\n",
    "DataIngestionConfig = DataIngestion(\n",
    "    ROOT_DIR_PATH = Path(\n",
    "        constants[\"DataIngestion\"].ROOT_DIR_NAME),\n",
    "    REPORTS_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataIngestion\"].ROOT_DIR_NAME,\n",
    "        constants[\"DataIngestion\"].REPORTS_DIR_NAME)),\n",
    "    DATA_ROOT_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataIngestion\"].ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].DATA_ROOT_DIR_NAME)),\n",
    "    INGESTION_ROOT_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataIngestion\"].ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].DATA_ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].INGESTION_ROOT_DIR_NAME)),\n",
    "    RAW_DATA_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataIngestion\"].ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].DATA_ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].INGESTION_ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].RAW_DATA_DIR_NAME)),\n",
    "    SCHEMA_DATA_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataIngestion\"].ROOT_DIR_NAME,\n",
    "        constants[\"DataIngestion\"].DATA_ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].INGESTION_ROOT_DIR_NAME, \n",
    "        constants[\"DataIngestion\"].SCHEMA_DATA_DIR_NAME)),\n",
    ")\n",
    "\n",
    "DataTransformationConfig = DataTransformation(\n",
    "    ROOT_DIR_PATH = Path(\n",
    "        constants[\"DataTransformation\"].ROOT_DIR_NAME),\n",
    "    DATA_ROOT_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataTransformation\"].ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].DATA_ROOT_DIR_NAME)),\n",
    "    TRANSFORMATION_ROOT_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataTransformation\"].ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].DATA_ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].TRANSFORMATION_ROOT_DIR_NAME)),\n",
    "    PARSED_DATA_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataTransformation\"].ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].DATA_ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].TRANSFORMATION_ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].PARSED_DATA_DIR_NAME)),\n",
    "    STRUCTURED_DATA_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataTransformation\"].ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].DATA_ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].TRANSFORMATION_ROOT_DIR_NAME,\n",
    "        constants[\"DataTransformation\"].STRUCTURED_DATA_DIR_NAME)),\n",
    "    TRAIN_DATA_DIR_PATH = Path(os.path.join(\n",
    "        constants[\"DataTransformation\"].TRAIN_DATA_DIR_NAME))\n",
    ")\n",
    "\n",
    "__all__ = [\"DataIngestionConfig\", \"DataTransformationConfig\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188fd54",
   "metadata": {},
   "source": [
    "# components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea31113",
   "metadata": {},
   "source": [
    "### schema/job_description.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a885343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional \n",
    "\n",
    "\n",
    "class JobDescription(BaseModel):\n",
    "    job_title: str = Field(description=\"The job title\")\n",
    "    company_name: str = Field(description=\"The company name\")\n",
    "    location: str = Field(description=\"Job location\")\n",
    "    job_type: str = Field(description=\"Employment type (full-time, part-time, etc.)\")\n",
    "    experience_level: str = Field(description=\"Required experience level\")\n",
    "    job_description: str = Field(description=\"Complete job description text\")\n",
    "    requirements: str = Field(description=\"Job requirements and qualifications\")\n",
    "    responsibilities: str = Field(description=\"Key responsibilities\")\n",
    "    salary_range: Optional[str] = Field(description=\"Salary information if available\")\n",
    "    posted_date: Optional[str] = Field(description=\"When the job was posted\")\n",
    "\n",
    "\n",
    "__all__ = [\"JobDescription\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "JobDescription.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88de6ef",
   "metadata": {},
   "source": [
    "### schema/resume.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66786a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, EmailStr\n",
    "from typing import List, Optional\n",
    "from datetime import date\n",
    "\n",
    "class PersonalInfo(BaseModel):\n",
    "    name: str = Field(..., min_length=1, max_length=100)\n",
    "    email: EmailStr\n",
    "    phone: str = Field(..., pattern=r'^[\\+]?[1-9][\\d]{0,15}$')\n",
    "    location: str\n",
    "    linkedin: Optional[str] = None\n",
    "\n",
    "class ProfessionalSummary(BaseModel):\n",
    "    headline: str\n",
    "    summary: str\n",
    "    total_experience_years: int = Field(..., ge=0, le=50)\n",
    "    career_level: str = Field(..., pattern=r'^(entry|junior|mid|senior|executive)$')\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    title: str\n",
    "    company: str\n",
    "    start_date: date \n",
    "    end_date: date\n",
    "    duration_months: int = Field(..., ge=0)\n",
    "    responsibilities: List[str]\n",
    "    achievements: List[str]\n",
    "    technologies_used: List[str]\n",
    "\n",
    "class Skills(BaseModel):\n",
    "    technical: List[str]\n",
    "    soft: List[str]\n",
    "    certifications: List[str]\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: str\n",
    "    institution: str\n",
    "    graduation_year: int = Field(..., ge=1950, le=2025)\n",
    "    gpa: Optional[str] = None\n",
    "\n",
    "class ResumeSchema(BaseModel):\n",
    "    personal_info: PersonalInfo\n",
    "    professional_summary: ProfessionalSummary\n",
    "    work_experience: List[WorkExperience]\n",
    "    skills: Skills\n",
    "    education: List[Education]\n",
    "    keywords: List[str]\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"personal_info\": {\n",
    "                    \"name\": \"John Doe\",\n",
    "                    \"email\": \"john.doe@email.com\",\n",
    "                    \"phone\": \"+1-555-0123\",\n",
    "                    \"location\": \"New York, NY\",\n",
    "                    \"linkedin\": \"linkedin.com/in/johndoe\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "__all__ = [\"ResumeSchema\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResumeSchema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd5bbd",
   "metadata": {},
   "source": [
    "# schema/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update __all__\n",
    "\n",
    "# from .job_description import *\n",
    "# from .resume import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaaf255",
   "metadata": {},
   "source": [
    "### parser/job_description.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bd459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from firecrawl import Firecrawl\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class JobDescriptionParser:\n",
    "    def __init__(self, firecrawl_api_key:str = None) -> None:\n",
    "        if not firecrawl_api_key:\n",
    "            load_dotenv()\n",
    "            firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "        if not firecrawl_api_key:\n",
    "            raise ValueError(f\"argument \\'firecrawl_api_key\\' is having value \\'{firecrawl_api_key}\\'\")\n",
    "        self.firecrawl = Firecrawl(api_key=firecrawl_api_key)\n",
    "\n",
    "    def extract_job_description(self, url:str):\n",
    "        \"\"\"\n",
    "        Extract job description using Firecrawl's AI-powered extraction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Method 1: Using structured extraction with schema\n",
    "            result = self.firecrawl.scrape(\n",
    "                url,\n",
    "                formats=[{\n",
    "                    \"type\": \"json\",\n",
    "                    \"schema\": JobDescription.model_json_schema()\n",
    "                }],\n",
    "                only_main_content=True,\n",
    "                timeout=30000\n",
    "            )\n",
    "            \n",
    "            if result.get('success'):\n",
    "                return result['data']['json']\n",
    "            else:\n",
    "                print(f\"Firecrawl extraction failed: {result}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Firecrawl: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_job_description_with_prompt(self, url:str):\n",
    "        \"\"\"\n",
    "        Alternative method using natural language prompt\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.firecrawl.scrape(\n",
    "                url,\n",
    "                formats=[{\n",
    "                    \"type\": \"json\",\n",
    "                    \"prompt\": \"\"\"Extract the following information from this job posting:\n",
    "                    - Job title\n",
    "                    - Company name\n",
    "                    - Location\n",
    "                    - Job type (full-time, part-time, etc.)\n",
    "                    - Experience level required\n",
    "                    - Complete job description\n",
    "                    - Requirements and qualifications\n",
    "                    - Key responsibilities\n",
    "                    - Salary range (if mentioned)\n",
    "                    - Posted date (if available)\n",
    "                    \n",
    "                    Return as structured JSON.\"\"\"\n",
    "                }],\n",
    "                only_main_content=True\n",
    "            )\n",
    "            \n",
    "            if result.get('success'):\n",
    "                return result['data']['json']\n",
    "            else:\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def parse(self, url:str) -> JobDescription | Dict | None:\n",
    "        job_data = self.extract_job_description(url)\n",
    "        if not job_data:\n",
    "            job_data = self.extract_job_description_with_prompt(url)\n",
    "        return job_data\n",
    "    \n",
    "\n",
    "__all__ = [\"JobDescriptionParser\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3fc8f",
   "metadata": {},
   "source": [
    "# parser/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update __all__\n",
    "\n",
    "# from .pdf import * \n",
    "# from .docx import * \n",
    "# from .html import * \n",
    "# from .job_description import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23368d1",
   "metadata": {},
   "source": [
    "### data_transformation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from src.ats import logging\n",
    "from src.ats.components.parsers import *\n",
    "from src.ats.utils import save_file, dump_json, create_dirs\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from dotenv import load_dotenv \n",
    "from datetime import datetime\n",
    "import os \n",
    "\n",
    "\n",
    "class DataTransformationComponents:\n",
    "    def __init__(self, data_ingestion_config: DataIngestion, data_transformation_config: DataTransformation, llm:BaseChatModel = None) -> None:\n",
    "        self.data_ingestion_config = data_ingestion_config\n",
    "        self.data_transformation_config = data_transformation_config\n",
    "        self.llm = llm\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        load_dotenv()\n",
    "        if not self.llm:\n",
    "            self.llm = ChatGoogleGenerativeAI(model=os.getenv(\"LLM\"))\n",
    "\n",
    "    def __parse(self, info:Dict[str, Dict[str, Path | Any]]) -> Dict[str, str]: \n",
    "        \"\"\"parse the data from file, supported extentions ---> [ .pdf, .docx, .html ]\n",
    "\n",
    "        Args:\n",
    "            info (Dict[str, Dict[str, Path | Any]]): \n",
    "                there should be a key named \\'path\\' containing path to the file in the sub dictionary\n",
    "\n",
    "                key = name of file \n",
    "\n",
    "                value = dictionary \n",
    "\n",
    "                example:\n",
    "                info = {\n",
    "                    \"xyz.pdf\" : {\n",
    "                        \"path\": \"path/to/the/file\",\n",
    "                        ...\n",
    "                    }, \n",
    "                    \"abc.docx\": {\n",
    "                        \"path\": \"path/to/the/file\",\n",
    "                        ...\n",
    "                    }\n",
    "                    ...\n",
    "                } \n",
    "                output = __parse(info)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if provided file with uncompatable format in argument \n",
    "\n",
    "        Returns:\n",
    "            Dict: \n",
    "            key = name of file \n",
    "\n",
    "            value = string object of parsed data  \n",
    "\n",
    "            example:\n",
    "            output = __parse(info)\n",
    "            output = {\n",
    "                \"xyz.pdf\" : \"string_parsed_data_of_xyz.pdf\", \n",
    "                \"abc.docx\": \"string_parsed_data_of_abc.docx\", \n",
    "                ...\n",
    "            } \n",
    "            name = output.keys()[0]\n",
    "            data = output[name] \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In __parse\")\n",
    "            output = {}\n",
    "            for file_name in info.keys():\n",
    "                logging.info(f\"parsing \\'{file_name}\\'\")\n",
    "                ext = os.path.splitext(file_name)[1].lower()\n",
    "                # get parser \n",
    "                if ext == \".pdf\":\n",
    "                    parser = PDFParser()\n",
    "                elif ext == \".docx\":\n",
    "                    parser = DOCXParser()\n",
    "                elif ext == \".html\":\n",
    "                    parser = HTMLParser()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file type: {file_name}\") \n",
    "                logging.info(f\"using \\'{parser.__class__.__name__}\\'\")\n",
    "                path = info[file_name][\"path\"]\n",
    "                logging.info(f\"path of file for parsing \\'{path}\\'\") \n",
    "                extracted_data = parser.parse(path)\n",
    "                logging.info(\"parsing complete.\")\n",
    "                # save file to local \n",
    "                output_path = os.path.join(self.data_transformation_config.PARSED_DATA_DIR_PATH, file_name)\n",
    "                save_file(extracted_data, output_path) \n",
    "                logging.info(f\"parsing complete for file \\'{file_name}\\'\")\n",
    "                output[file_name] = extracted_data \n",
    "            logging.info(\"Out __parse\") \n",
    "            return output \n",
    "        except Exception as e: \n",
    "            logging.error(e) \n",
    "            raise CustomException(e, sys) \n",
    "        \n",
    "    def __extract_keyword(self, data:Dict[str, str]) -> Dict[str, ResumeSchema]: \n",
    "        \"\"\"extract structed output from parsed string data\n",
    "\n",
    "        Args:\n",
    "            data (Dict[str, str]): dictionary with name of file as keys and parsed data as value of respective keys\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            Dict[ str, ResumeSchema ]: dictionary with name of file as keys and structured data as value of respective keys\n",
    "            \n",
    "            example:\n",
    "            output = __extract_keyword(data)\n",
    "            output = {\n",
    "                        \"personal_info\": {\n",
    "                            \"name\":\"\",\n",
    "                            \"email\":\"\",\n",
    "                            \"phone\":\"\",\n",
    "                            \"location\":\"\",\n",
    "                            \"linkedin\":\"\",\n",
    "                        }, \n",
    "                        \"professional_summary\": {\n",
    "                            \"headline\":\"\", \n",
    "                            \"summary\":\"\", \n",
    "                            \"total_experience_years\":\"\", \n",
    "                            \"career_level\":\"\", \n",
    "                        }, \n",
    "                        \"work_experience\": [\n",
    "                            {\n",
    "                                \"title\":\"\",\n",
    "                                \"company\":\"\",\n",
    "                                \"start_date\":\"\",\n",
    "                                \"end_date\":\"\",\n",
    "                                \"duration_months\":\"\",\n",
    "                                \"responsibilities\":\"\",\n",
    "                                \"achievements\":\"\",\n",
    "                                \"technologies_used\":\"\",\n",
    "                            }\n",
    "                        ], \n",
    "                        \"skills\": {\n",
    "                            \"technical\": \"\",\n",
    "                            \"soft\": \"\",\n",
    "                            \"certifications\": \"\",\n",
    "                        }, \n",
    "                        \"education\": [\n",
    "                            {\n",
    "                                \"degree\":\"\",\n",
    "                                \"institution\":\"\",\n",
    "                                \"graduation_year\":\"\",\n",
    "                                \"gpa\":\"\",\n",
    "                            }\n",
    "                        ], \n",
    "                        \"keywords\": [\"\", \"\", ...], \n",
    "                    }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            output = {}\n",
    "            train_data = {\n",
    "                \"X\":[], \n",
    "                \"y\":[]\n",
    "            }\n",
    "            for file_name in data.keys():\n",
    "                parsed_data = data[file_name]\n",
    "                structured_parsed_data = self.llm.with_structured_output(ResumeSchema).invoke(parsed_data)\n",
    "                output[file_name] = structured_parsed_data\n",
    "                # persist transformed data to disk\n",
    "                path = os.path.join(self.data_transformation_config.STRUCTURED_DATA_DIR_PATH, file_name)\n",
    "                dump_json(structured_parsed_data, path)\n",
    "                # create training data\n",
    "                train_data[\"X\"].append(parsed_data)\n",
    "                train_data[\"y\"].append(structured_parsed_data)\n",
    "            # persist structed data to disk\n",
    "            train_data_path = os.path.join(self.data_transformation_config.TRAIN_DATA_DIR_PATH, f\"{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")}.json\")\n",
    "            dump_json(train_data, train_data_path)\n",
    "            return output\n",
    "        except Exception as e: \n",
    "            logging.error(e) \n",
    "            raise CustomException(e, sys)  \n",
    "        \n",
    "    def _main(self, info:Dict[str, Dict[str, Path | Any]]) -> Dict[str, ResumeSchema]:\n",
    "        \"\"\"runs the transformation pipeline\n",
    "\n",
    "        Args:\n",
    "            info (Dict[str, Dict[str, Path  |  Any]]): \n",
    "                there should be a key named \\'path\\' containing path to the file in the sub dictionary\n",
    "\n",
    "                key = name of file \n",
    "\n",
    "                value = dictionary \n",
    "\n",
    "                example:\n",
    "                info = {\n",
    "                    \"xyz.pdf\" : {\n",
    "                        \"path\": \"path/to/the/file\",\n",
    "                        ...\n",
    "                    }, \n",
    "                    \"abc.docx\": {\n",
    "                        \"path\": \"path/to/the/file\",\n",
    "                        ...\n",
    "                    }\n",
    "                    ...\n",
    "                } \n",
    "                output = _main(info)\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Dict[ str, ResumeSchema ]: dictionary with name of file as keys and structured data as value of respective keys\n",
    "\n",
    "            example:\n",
    "            output = _main(data)\n",
    "            output = {\n",
    "                        \"personal_info\": {\n",
    "                            \"name\":\"\",\n",
    "                            \"email\":\"\",\n",
    "                            \"phone\":\"\",\n",
    "                            \"location\":\"\",\n",
    "                            \"linkedin\":\"\",\n",
    "                        }, \n",
    "                        \"professional_summary\": {\n",
    "                            \"headline\":\"\", \n",
    "                            \"summary\":\"\", \n",
    "                            \"total_experience_years\":\"\", \n",
    "                            \"career_level\":\"\", \n",
    "                        }, \n",
    "                        \"work_experience\": [\n",
    "                            {\n",
    "                                \"title\":\"\",\n",
    "                                \"company\":\"\",\n",
    "                                \"start_date\":\"\",\n",
    "                                \"end_date\":\"\",\n",
    "                                \"duration_months\":\"\",\n",
    "                                \"responsibilities\":\"\",\n",
    "                                \"achievements\":\"\",\n",
    "                                \"technologies_used\":\"\",\n",
    "                            }\n",
    "                        ], \n",
    "                        \"skills\": {\n",
    "                            \"technical\": \"\",\n",
    "                            \"soft\": \"\",\n",
    "                            \"certifications\": \"\",\n",
    "                        }, \n",
    "                        \"education\": [\n",
    "                            {\n",
    "                                \"degree\":\"\",\n",
    "                                \"institution\":\"\",\n",
    "                                \"graduation_year\":\"\",\n",
    "                                \"gpa\":\"\",\n",
    "                            }\n",
    "                        ], \n",
    "                        \"keywords\": [\"\", \"\", ...], \n",
    "                    }\n",
    "        \"\"\"\n",
    "        # create required dir's \n",
    "        create_dirs(self.data_transformation_config.ROOT_DIR_PATH)\n",
    "        create_dirs(self.data_transformation_config.DATA_ROOT_DIR_PATH)\n",
    "        create_dirs(self.data_transformation_config.TRANSFORMATION_ROOT_DIR_PATH)\n",
    "        create_dirs(self.data_transformation_config.PARSED_DATA_DIR_PATH)\n",
    "        create_dirs(self.data_transformation_config.STRUCTURED_DATA_DIR_PATH)\n",
    "        create_dirs(self.data_transformation_config.TRAIN_DATA_DIR_PATH)\n",
    "        # start steps\n",
    "        data = self.__parse(info)\n",
    "        return self.__extract_keyword(data)\n",
    "    \n",
    "\n",
    "__all__ = [\"DataTransformationComponents\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace00a13",
   "metadata": {},
   "source": [
    "### __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79759ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update __all__ \n",
    "\n",
    "from src.ats.components.data_ingestion import * \n",
    "from src.ats.components.data_transformation import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4337e",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9c88b",
   "metadata": {},
   "source": [
    "### __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update __all__\n",
    "\n",
    "from src.ats.components import * \n",
    "# from src.ats.config.builder import * \n",
    "from dataclasses import dataclass \n",
    "from typing import List, Dict \n",
    "from fastapi import UploadFile \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataIngestionPipeline: \n",
    "    \"\"\"pipeline for process of data ingestion \n",
    "    \"\"\"\n",
    "    def _run(self, files: List[UploadFile]) -> Dict[str, str]: \n",
    "        \"\"\"runs data ingestion pipeline and returns the output\n",
    "\n",
    "        Args:\n",
    "            files (List[UploadFile]): list object of fastapi.UploadFile / files that have been uploaded\n",
    "\n",
    "        Returns:\n",
    "            Dict: shown below\n",
    "\n",
    "            example:\n",
    "            schema = _main(info)\n",
    "            schema = {\n",
    "                \"path\": path/of/the/file/in/disk,\n",
    "                \"size\": size of the file in disk,\n",
    "                \"binary_content_size\": total number of binary digits inside file (len(origin_data)),\n",
    "                \"base64_content_size\": total number of base64 digits after converting from bytes to base64 string (len(base64_data))\n",
    "            } \n",
    "            file_path = schema[\"path\"]\n",
    "            file_size = schema[\"size\"]\n",
    "            original_content_size = schema[\"binary_content_size\"]\n",
    "            converted_content_size = schema[\"base64_content_size\"]\n",
    "        \"\"\"\n",
    "        components = DataIngestionComponents(DataIngestionConfig) \n",
    "        return components._main(files) \n",
    "    \n",
    "@dataclass \n",
    "class DataTransformationPipeline: \n",
    "    \"\"\"pipeline for process of data transformation \n",
    "    \"\"\"\n",
    "    def _run(self, schema:Dict[str, Dict[str, Path | Any]]) -> Dict[str, ResumeSchema]: \n",
    "        \"\"\"runs data transformation pipeline and returns the output\n",
    "\n",
    "        Args:\n",
    "            Args:\n",
    "            schema (Dict[str, Dict[str, Path  |  Any]]): \n",
    "                there should be a key named \\'path\\' containing path to the file in the sub dictionary\n",
    "\n",
    "                key = name of file \n",
    "\n",
    "                value = dictionary \n",
    "\n",
    "                example:\n",
    "                schema = {\n",
    "                    \"xyz.pdf\" : {\n",
    "                        \"path\": \"path/to/the/file\",\n",
    "                        ...\n",
    "                    }, \n",
    "                    \"abc.docx\": {\n",
    "                        \"path\": \"path/to/the/file\",\n",
    "                        ...\n",
    "                    }\n",
    "                    ...\n",
    "                } \n",
    "                pipeline = DataTransformationPipeline()\n",
    "                output = pipeline._run(schema)\n",
    "\n",
    "        Returns:\n",
    "            Dict[ str, ResumeSchema ]: dictionary with name of file as keys and structured data as value of respective keys\n",
    "\n",
    "            example:\n",
    "            pipeline = DataTransformationPipeline()\n",
    "            output = pipeline._run(schema)\n",
    "            output = {\n",
    "                        \"personal_info\": {\n",
    "                            \"name\":\"\",\n",
    "                            \"email\":\"\",\n",
    "                            \"phone\":\"\",\n",
    "                            \"location\":\"\",\n",
    "                            \"linkedin\":\"\",\n",
    "                        }, \n",
    "                        \"professional_summary\": {\n",
    "                            \"headline\":\"\", \n",
    "                            \"summary\":\"\", \n",
    "                            \"total_experience_years\":\"\", \n",
    "                            \"career_level\":\"\", \n",
    "                        }, \n",
    "                        \"work_experience\": [\n",
    "                            {\n",
    "                                \"title\":\"\",\n",
    "                                \"company\":\"\",\n",
    "                                \"start_date\":\"\",\n",
    "                                \"end_date\":\"\",\n",
    "                                \"duration_months\":\"\",\n",
    "                                \"responsibilities\":\"\",\n",
    "                                \"achievements\":\"\",\n",
    "                                \"technologies_used\":\"\",\n",
    "                            }\n",
    "                        ], \n",
    "                        \"skills\": {\n",
    "                            \"technical\": \"\",\n",
    "                            \"soft\": \"\",\n",
    "                            \"certifications\": \"\",\n",
    "                        }, \n",
    "                        \"education\": [\n",
    "                            {\n",
    "                                \"degree\":\"\",\n",
    "                                \"institution\":\"\",\n",
    "                                \"graduation_year\":\"\",\n",
    "                                \"gpa\":\"\",\n",
    "                            }\n",
    "                        ], \n",
    "                        \"keywords\": [\"\", \"\", ...], \n",
    "                    }\n",
    "        \"\"\"\n",
    "        components = DataTransformationComponents(DataIngestionConfig, DataTransformationConfig) \n",
    "        return components._main(schema) \n",
    "\n",
    "\n",
    "__all__ = [\"DataIngestionPipeline\", \"DataTransformationPipeline\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a8408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ats-score-checker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
